import os
import sys
import threading
import pyaudio
import json
import socket
from vosk import Model, KaldiRecognizer
from PyQt5.QtGui import QIcon
from PyQt5.QtCore import Qt, QSize
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel
import time
import cv2
from ultralytics import YOLO

class VoiceRecognizer(QWidget):
    def __init__(self):
        super().__init__()

        self.setWindowTitle("ë‹¹ì‹ ì˜ ì¹œêµ¬ BOOGIE")
        self.setFixedSize(400, 400)

        self.init_ui()

        # ìŒì„± ì¸ì‹ ì„¤ì •
        self.model_path = os.path.join(os.path.dirname(__file__), "models/vosk-model-en-us-0.42-gigaspeech")
        self.yolo = YOLO(os.path.join(os.path.dirname(__file__), "models/best.onnx"))
        self.sample_rate = 8000
        self.chunk_size = 1024
        self.running = False

        # TCP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.server_host = "192.168.0.95"  # ì‹¤ì œ ì„œë²„ IP
        self.server_port = 12345
        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                self.client_socket.connect((self.server_host, self.server_port))
                print(f"Connected to server at {self.server_host}:{self.server_port}")
                break
            except Exception as e:
                print(f"Attempt {attempt + 1} failed: {e}")
                if attempt == max_attempts - 1:
                    self.label.setText(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
                    self.mic_button.setEnabled(False)
                time.sleep(1)

        if not os.path.exists(self.model_path):
            self.label.setText("ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.mic_button.setEnabled(False)
        else:
            self.model = Model(self.model_path)
            self.rec = KaldiRecognizer(self.model, self.sample_rate)

    def init_ui(self):
        layout = QVBoxLayout()
        layout.setAlignment(Qt.AlignCenter)

        self.label = QLabel("ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•˜ì„¸ìš”.")
        self.label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.label)

        self.mic_button = QPushButton()
        self.mic_button.setIcon(QIcon.fromTheme("microphone"))
        self.mic_button.setText("ë§ˆì´í¬")
        self.mic_button.setIconSize(QSize(50, 50))
        self.mic_button.setFixedHeight(40)
        self.mic_button.clicked.connect(self.toggle_recognition)
        self.mic_button.setStyleSheet("""
            QPushButton {
                background-color: #2196F3;
                color: white;
                border: none;
                border-radius: 20px;
                font-weight: bold;
                padding: 8px 16px;
            }
            QPushButton:hover {
                background-color: #1976D2;
            }
        """)
        layout.addWidget(self.mic_button, alignment=Qt.AlignCenter)

        self.result_label = QLabel("")
        self.result_label.setAlignment(Qt.AlignCenter)
        self.result_label.setStyleSheet("color: green; font-size: 16px;")
        layout.addWidget(self.result_label)

        self.setLayout(layout)

    def toggle_recognition(self):
        if not self.running:
            self.start_recognition()
        else:
            self.stop_recognition()

    def start_recognition(self):
        self.running = True
        self.mic_button.setText("ì¤‘ì§€")
        self.label.setText("ìŒì„± ì¸ì‹ ì¤‘... ë§ˆì´í¬ì— ë§í•˜ì„¸ìš”.")
        self.result_label.setText("")
        
        # ì„œë²„ì— ìŒì„± ì¸ì‹ ì‹œì‘ ì‹ í˜¸ ì „ì†¡
        try:
            self.client_socket.send(json.dumps({"command": "start_recognition"}).encode('utf-8'))
            print("Sent start_recognition signal")
        except Exception as e:
            print(f"Failed to send start_recognition signal: {e}")
            self.label.setText(f"ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
            self.running = False
            self.mic_button.setText("ğŸ™ï¸")
            return

        self.thread = threading.Thread(target=self.recognize)
        self.thread.start()

    def stop_recognition(self):
        self.running = False
        self.mic_button.setText("ë§ˆì´í¬")
        self.label.setText("ìŒì„± ì¸ì‹ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def recognize(self):
        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16,
                        channels=1,
                        rate=self.sample_rate,
                        input=True,
                        frames_per_buffer=self.chunk_size)

        stream.start_stream()

        try:
            while self.running:
                data = stream.read(self.chunk_size, exception_on_overflow=False)
                if self.rec.AcceptWaveform(data):
                    result_json = self.rec.Result()
                    result_dict = json.loads(result_json)
                    self.text_process(result_dict.get("text", ""))
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.result_label.setText(f"ì˜¤ë¥˜: {e}")
        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    def text_process(self, text):
        print(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
        token = text.split()
        for word in token:
            if word in ["boogie", "cookie", "pookie", "ookie"]:
                self.result_label.setText("ì£¼ì¸ë‹˜ ì˜¤ëŠ˜ì˜ ëŠë‚Œì€?")
                emotion_data = self.yolo_emotion_detection()
                if emotion_data:
                    try:
                        # ê°ì • ë°ì´í„° ì „ì†¡
                        emotion_json = json.dumps(emotion_data)
                        self.client_socket.send(emotion_json.encode('utf-8'))
                        print(f"ê°ì • ì „ì†¡: {emotion_json}")
                        # ê°ì • ì „ì†¡ í›„ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
                        time.sleep(0.1)  # ì§§ì€ ì§€ì—°
                        self.client_socket.send(json.dumps({"command": "end_recognition"}).encode('utf-8'))
                        print("Sent end_recognition signal")
                    except Exception as e:
                        print(f"ê°ì • ì „ì†¡ ì‹¤íŒ¨: {e}")
                        self.result_label.setText(f"ì „ì†¡ ì‹¤íŒ¨: {e}")
                self.stop_recognition()
                return
        self.result_label.setText("")

    def yolo_emotion_detection(self):
        cap = cv2.VideoCapture(0)
        ret, frame = cap.read()
        if not ret:
            print("Failed to grab frame.")
            self.result_label.setText("ì¹´ë©”ë¼ í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨")
            cap.release()
            return None

        gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray_image_3d = cv2.merge([gray_image, gray_image, gray_image])

        results = self.yolo(gray_image_3d)
        result = results[0]

        emotion_data = None
        for box in result.boxes:
            conf = float(box.conf[0])
            class_id = int(box.cls[0])
            class_name = self.yolo.names[class_id]
            emotion_data = {
                "emotion": class_name,
                "confidence": conf
            }
            if class_name == "Sad":
                self.result_label.setText("ë„ˆë¬´ ìŠ¬í”ˆ í‘œì • ì§“ì§€ ë§ì•„ì£¼ì„¸ìš”.")
            elif class_name == "Happy":
                self.result_label.setText("ì˜¤ëŠ˜ë„ í™”ì´íŒ…ì…ë‹ˆë‹¤.")
            elif class_name == "Neutral":
                self.result_label.setText("ì›ƒì–´ë³´ë©´ ì–´ë–¨ê¹Œìš”???")

            break

        cap.release()
        return emotion_data

    def closeEvent(self, event):
        self.running = False
        try:
            self.client_socket.send(json.dumps({"command": "end_recognition"}).encode('utf-8'))
            print("Sent end_recognition signal on close")
        except:
            pass
        self.client_socket.close()
        event.accept()

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = VoiceRecognizer()
    window.show()
    sys.exit(app.exec_())